{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pymystem3\n",
    "\n",
    "!pip install pymorphy2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем книгу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('book.txt', 'r',  encoding=\"utf-8\")\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрабатываем при помощи MyStem и сохраняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pymystem3 import Mystem\n",
    "from pprint import pprint\n",
    "import json\n",
    "m = Mystem()\n",
    "ana = m.analyze(text)\n",
    "with open('mystem.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ana, f, ensure_ascii = False, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем при помощи nltk, удаляем знаки препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обрабатываем при помощи pymorphy и сохраняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "l = []\n",
    "an = []\n",
    "lemmas = []\n",
    "for word in words:\n",
    "    s = []\n",
    "    anali = morph.parse(word)\n",
    "    an.append(anali[0])\n",
    "    lemma = anali[0].normal_form\n",
    "    lemmas.append (lemma)\n",
    "    pos = anali[0].tag.POS\n",
    "    s.append(lemma)\n",
    "    s.append(pos)\n",
    "    l.append(s)\n",
    "with open('pymorphy.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(l, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обрабатываем при помощи pymorphy, создаем частотный словарь для частей речи, находим долю для каждой части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for word in an:\n",
    "    if word.tag.POS in d:\n",
    "        d[word.tag.POS] += 1\n",
    "    else:\n",
    "        d[word.tag.POS] = 1\n",
    "for pos in d:\n",
    "    print(pos, ' - ', d[pos]/len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем частотный словарь наречий и глаголов, сортируем по частотности, выводим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "v = {}\n",
    "a = {}\n",
    "for word in an:\n",
    "    if word.tag.POS == 'VERB':\n",
    "        if word.normal_form in v:\n",
    "            v[word.normal_form] += 1\n",
    "        else:\n",
    "            v[word.normal_form] = 1\n",
    "    if word.tag.POS == 'ADVB':\n",
    "        if word.normal_form in v:\n",
    "            a[word.normal_form] += 1\n",
    "        else:\n",
    "            a[word.normal_form] = 1\n",
    "print('Глаголы:')\n",
    "sorted_v = sorted(v.items(), key=operator.itemgetter(1))\n",
    "for i in range(20):\n",
    "    k = (len(sorted_v) - i - 1)\n",
    "    print(sorted_v[k][0])\n",
    "print()\n",
    "print('Наречия:')\n",
    "sorted_a = sorted(a.items(), key=operator.itemgetter(1))\n",
    "for i in range(20):\n",
    "    k = (len(sorted_a) - i - 1)\n",
    "    print(sorted_a[k][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим и составляем все биграммы и триграммы, делаем частотный словарь, сортируем и выводим самые частотные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u = []\n",
    "r = []\n",
    "b = {}\n",
    "t = {}\n",
    "print('Биграммы:')\n",
    "bigrm = nltk.bigrams(lemmas)\n",
    "for i in bigrm:\n",
    "    bi = i[0] + ' ' + i[1]\n",
    "    u.append(bi)\n",
    "for words in u:\n",
    "    if words in b:\n",
    "        b[words] += 1\n",
    "    else:\n",
    "        b[words] = 1\n",
    "sorted_b = sorted(b.items(), key=operator.itemgetter(1))\n",
    "for i in range(25):\n",
    "    k = (len(sorted_b) - i - 1)\n",
    "    print(sorted_b[k][0])\n",
    "print()\n",
    "print('Триграммы:')\n",
    "trgrm = nltk.trigrams(lemmas)\n",
    "for i in trgrm:\n",
    "    tr = i[0] + ' ' + i[1] + ' ' + i[2]\n",
    "    r.append(tr)\n",
    "for words in r:\n",
    "    if words in t:\n",
    "        t[words] += 1\n",
    "    else:\n",
    "        t[words] = 1\n",
    "sorted_t = sorted(t.items(), key=operator.itemgetter(1))\n",
    "for i in range(25):\n",
    "    k = (len(sorted_t) - i - 1)\n",
    "    print(sorted_t[k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
